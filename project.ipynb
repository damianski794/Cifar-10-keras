{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod, abstractstaticmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import (\n",
    "    Add,\n",
    "    BatchNormalization,\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Embedding,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    Layer,\n",
    "    LayerNormalization,\n",
    "    MaxPool2D,\n",
    "    MultiHeadAttention,\n",
    "    Normalization,\n",
    "    RandomContrast,\n",
    "    RandomFlip,\n",
    "    RandomRotation, # zamula\n",
    "    RandomTranslation,\n",
    "    RandomZoom,\n",
    "    Resizing\n",
    ")\n",
    "import keras_tuner as kt\n",
    "import tensorflow_addons as tfa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is detected:\n",
    "len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project:\n",
    "\n",
    "# np.random_seed(5) # uncommend for repetitive results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ba766",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train/255.0  # normalization\n",
    "x_test = x_test/255.0\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e484ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history, model_name):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history, model_name):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "def plot_history(history, model_name):\n",
    "    plot_accuracy(history, model_name)\n",
    "    plot_loss(history, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707736fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_id_to_names = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n",
    "\n",
    "cifar_names_to_id = {\n",
    "    \"airplane\": 0,\n",
    "    \"automobile\": 1,\n",
    "    \"bird\": 2,\n",
    "    \"cat\": 3,\n",
    "    \"deer\": 4,\n",
    "    \"dog\": 5,\n",
    "    \"frog\": 6,\n",
    "    \"horse\": 7,\n",
    "    \"ship\": 8,\n",
    "    \"truck\": 9,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPConfiguration(ABC):\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def build_hp_model(hp):\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_callbacks():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner_callbacks():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks\n",
    "\n",
    "class SuperSimpleConvModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters, \n",
    "                 dense_units,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "   \n",
    "    def build_hp_model(hp):\n",
    "        n_filters = hp.Int(\"n_filters\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SuperSimpleConvModel(\n",
    "            n_filters=n_filters, dense_units=dense_units\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SuperSimpleConvModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SuperSimpleConvModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "    \n",
    "    def get_best_model():\n",
    "        tuner = SuperSimpleConvModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return SuperSimpleConvModel.build_hp_model(best_parameters)\n",
    "    \n",
    "    \n",
    "####################\n",
    "class SimpleConvModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters_1,\n",
    "                 n_filters_2,\n",
    "                 dense_units_1,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters_1,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2D_2 = Conv2D(filters=n_filters_2,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units_1,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        x = self.conv2D_2(x)\n",
    "        x = self.max_pool2D_2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        n_filters_1 = hp.Int(\"n_filters_1\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        n_filters_2 = hp.Int(\"n_filters_2\", min_value=2, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units_1 = hp.Int(\"dense_units_1\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleConvModel(\n",
    "            n_filters_1=n_filters_1, n_filters_2=n_filters_2, dense_units_1=dense_units_1\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleConvModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleConvModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "###########################\n",
    "class SimpleConvDropoutModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters_1,\n",
    "                 n_filters_2,\n",
    "                 dense_units_1,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters_1,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2D_2 = Conv2D(filters=n_filters_2,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units_1,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "        \n",
    "        self.dropout = Dropout(0.25)\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        x = self.conv2D_2(x)\n",
    "        x = self.max_pool2D_2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        n_filters_1 = hp.Int(\"n_filters_1\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        n_filters_2 = hp.Int(\"n_filters_2\", min_value=2, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units_1 = hp.Int(\"dense_units_1\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleConvDropoutModel(\n",
    "            n_filters_1=n_filters_1, n_filters_2=n_filters_2, dense_units_1=dense_units_1\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleConvDropoutModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleConvDropoutModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SimpleConvDropoutModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "    \n",
    "#######################\n",
    "class SimpleConvDropoutAndBatchnormModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters_1,\n",
    "                 n_filters_2,\n",
    "                 dense_units_1,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters_1,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2D_2 = Conv2D(filters=n_filters_2,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units_1,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "        \n",
    "        self.dropout = Dropout(0.25)\n",
    "        self.batch_norm_1 = BatchNormalization()\n",
    "        self.batch_norm_2 = BatchNormalization()\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        # if training:\n",
    "        x = self.batch_norm_1(x)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        x = self.conv2D_2(x)\n",
    "#         if training:\n",
    "        x = self.batch_norm_2(x)\n",
    "        x = self.max_pool2D_2(x)\n",
    "        if training:\n",
    "            x = self.dropout(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        n_filters_1 = hp.Int(\"n_filters_1\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        n_filters_2 = hp.Int(\"n_filters_2\", min_value=2, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units_1 = hp.Int(\"dense_units_1\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleConvDropoutAndBatchnormModel(\n",
    "            n_filters_1=n_filters_1, n_filters_2=n_filters_2, dense_units_1=dense_units_1\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleConvDropoutAndBatchnormModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleConvDropoutAndBatchnormModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SimpleConvDropoutAndBatchnormModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1ee4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d34a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [SuperSimpleConvModel, SimpleConvModel] # tutaj dorzuć trzeci model do listy\n",
    "\n",
    "# HYPERPARAMETER TUNING\n",
    "for model_class in model_classes:\n",
    "    print(model_class)\n",
    "    # break\n",
    "    tuner = model_class.get_tuner()\n",
    "    #print(tuner)\n",
    "    tuner.search(x_train, y_train, batch_size=256, epochs=50, validation_data=(x_validation, y_validation), callbacks=model_class.get_tuner_callbacks())\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0068837",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96979b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cebfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#model_classes = [SimpleConvDropoutAndBatchnormModel]\n",
    "model_classes = [SimpleConvModel]\n",
    "\n",
    "# accuracy for every model (for training, validation and test set)\n",
    "accuracy_results = {\n",
    "    model_class.__name__ + \"_\" + str_type: [] for model_class in model_classes for str_type in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "\n",
    "print(accuracy_results)\n",
    "\n",
    "for model_class in model_classes:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(best_hps.values)\n",
    "    #best_hps.\n",
    "    \n",
    "    \n",
    "    for replication in range(1):\n",
    "        print(f'Starting {replication} iteration')\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        ## DAMIAN 26.03\n",
    "        #best_model = model_class(n_filters_1=32,\n",
    "        #                                    n_filters_2=32,\n",
    "        #                                    dense_units_1=64)\n",
    "        #best_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        \n",
    "        ## DAMIAN 26.03\n",
    "        print(best_model)\n",
    "        history = best_model.fit(x_train, y_train, epochs=200, validation_data=(x_validation, y_validation), batch_size=256, callbacks=model_class.get_callbacks(), verbose=1)\n",
    "        \n",
    "        ## DAMIAN \n",
    "        best_model = keras.models.load_model(f\"models/{model_class.__name__}\")\n",
    "   # TODO: dorzucić wczytywanie wag najlepszego modelu (optymalizacja pod kątem max validation_accuracy)     \n",
    "        print(f\"model name: {model_class.__name__}\")\n",
    "        print(f\"iteration: {replication}\")\n",
    "        train_loss, train_accuracy = best_model.evaluate(x=x_train, y=y_train)\n",
    "        accuracy_results[model_class.__name__ + \"_train\"].append(train_accuracy)\n",
    "        validation_loss, validation_accuracy = best_model.evaluate(x=x_validation, y=y_validation)\n",
    "        accuracy_results[model_class.__name__ + \"_validation\"].append(validation_accuracy)\n",
    "        test_loss, test_accuracy = best_model.evaluate(x=x_test, y=y_test)\n",
    "        accuracy_results[model_class.__name__ + \"_test\"].append(test_accuracy)\n",
    "        print(f\"train accuracy: {train_accuracy}\")\n",
    "        print(f\"validation accuracy: {validation_accuracy}\")\n",
    "        print(f\"test accuracy: {test_accuracy}\")\n",
    "        plot_history(history, model_class.__name__)\n",
    "        \n",
    "        print('============ NEW ITERATION ============')\n",
    "\n",
    "print(accuracy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyniki accuracy results\n",
    "for model_type, accuracy_list in accuracy_results.items():\n",
    "    avg = np.average(accuracy_list)\n",
    "    std = np.std(accuracy_list)\n",
    "    minimal = np.min(accuracy_list)\n",
    "    maximal = np.max(accuracy_list)\n",
    "    print(f\"mode: {model_type}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c6826de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# comitee models\n",
    "labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n",
    "def drawConfMatrix(model, x_test, y_test, labels):\n",
    "    predictions = model.predict(x_test, verbose=True)\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    confMat = ConfusionMatrixDisplay(confusion_matrix(y_test, predictions, normalize='true'), display_labels=labels)\n",
    "    print('accuracy_score', accuracy_score(y_test, predictions))\n",
    "    fig, ax = plt.subplots(figsize=(8,8))\n",
    "    confMat.plot(ax = ax,  xticks_rotation = 'vertical')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aabc4950",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = keras.models.load_model(f\"models/{model_class.__name__}\")\n",
    "\n",
    "drawConfMatrix(model, x_test, y_test, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "164646b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bird, Cat, Deer, Frog\n",
    "bird_idx = 2\n",
    "cat_idx = 3\n",
    "deer_idx = 4\n",
    "frog_idx = 6\n",
    "\n",
    "animal_filter_train = np.logical_or.reduce((y_train == bird_idx, y_train == cat_idx, y_train == deer_idx, y_train == frog_idx))[...,0]\n",
    "animal_filter_validation = np.logical_or.reduce((y_validation == bird_idx, y_validation == cat_idx, y_validation == deer_idx, y_validation == frog_idx))[...,0]\n",
    "animal_filter_test = np.logical_or.reduce((y_test == bird_idx, y_test == cat_idx, y_test == deer_idx, y_test == frog_idx))[...,0]\n",
    "\n",
    "x_train_animal = x_train[animal_filter_train]\n",
    "y_train_animal = y_train[animal_filter_train]\n",
    "x_validation_animal = x_validation[animal_filter_validation]\n",
    "y_validation_animal = y_validation[animal_filter_validation]\n",
    "x_test_animal = x_test[animal_filter_test]\n",
    "y_test_animal = y_test[animal_filter_test]\n",
    "print(animal_filter.shape)\n",
    "print(x_train.shape)\n",
    "print(x_train_animal.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa3829d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def map_from_animal_to_model(a):\n",
    "    if a==2:\n",
    "        return 0\n",
    "    if a==3:\n",
    "        return 1\n",
    "    if a==4:\n",
    "        return 2\n",
    "    if a==6:\n",
    "        return 3\n",
    "def map_from_model_to_animal(a):\n",
    "    if a==0:\n",
    "        return 2\n",
    "    if a==1:\n",
    "        return 3\n",
    "    if a==2:\n",
    "        return 4\n",
    "    if a==3:\n",
    "        return 6\n",
    "    \n",
    "vectorized_map_from_animal_to_model = np.vectorize(map_from_animal_to_model)\n",
    "vectorized_map_from_model_to_animal = np.vectorize(map_from_model_to_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53b2d971",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_animal = vectorized_map_from_animal_to_model(y_train_animal)\n",
    "y_validation_animal = vectorized_map_from_animal_to_model(y_validation_animal)\n",
    "y_test_animal = vectorized_map_from_animal_to_model(y_test_animal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79102bf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleConvAnimalModel(keras.Model):\n",
    "    def __init__(self, \n",
    "                 n_filters_1 =32,\n",
    "                 n_filters_2 = 32,\n",
    "                 dense_units_1 = 64,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters_1,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2D_2 = Conv2D(filters=n_filters_2,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units_1,activation='relu')\n",
    "        self.dense_2 = Dense(4,activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        x = self.conv2D_2(x)\n",
    "        x = self.max_pool2D_2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "animal_model = SimpleConvAnimalModel()\n",
    "animal_model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        \"models/Animal\",\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        # save_weights_only=True,\n",
    ")\n",
    "early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=20\n",
    "    )\n",
    "history = animal_model.fit(x_train_animal, y_train_animal, epochs=200, \n",
    "                           validation_data=(x_validation_animal, y_validation_animal), \n",
    "                           batch_size=256, \n",
    "                           callbacks=[checkpoint_callback, early_stopping_callback], \n",
    "                           verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8459b3f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_history(history, \"animal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af738053",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "predictions = model.predict(x_test, verbose=True)\n",
    "predictions = list(np.argmax(predictions, axis=1))\n",
    "final_results = deepcopy(predictions)\n",
    "for idx, single_prediction in enumerate(predictions):\n",
    "    if single_prediction==bird_idx or single_prediction==cat_idx or single_prediction==deer_idx or single_prediction==frog_idx:\n",
    "        image = x_test[idx]\n",
    "        image = image[np.newaxis, ...]\n",
    "        animal_prediction = animal_model.predict(image)\n",
    "        animal_prediction = int(np.argmax(animal_prediction, axis=1))\n",
    "        #print('animal_prediction', animal_prediction)\n",
    "        primary_model_prediction = map_from_model_to_animal(animal_prediction)\n",
    "        #print('primary_model_prediction', primary_model_prediction)\n",
    "        final_results[idx] = primary_model_prediction\n",
    "    if idx%100 == 0:\n",
    "        print('idx:', idx)\n",
    "    \n",
    "    #confMat = ConfusionMatrixDisplay(confusion_matrix(y_test, predictions, normalize='true'), display_labels=labels)\n",
    "    #fig, ax = plt.subplots(figsize=(8,8))\n",
    "    #confMat.plot(ax = ax,  xticks_rotation = 'vertical')\n",
    "comitee_y_results = np.array(final_results)\n",
    "\n",
    "confMat = ConfusionMatrixDisplay(confusion_matrix(y_test, comitee_y_results, normalize='true'), display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(8,8))\n",
    "confMat.plot(ax = ax,  xticks_rotation = 'vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47856a25",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "accuracy_score(y_test, comitee_y_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model_type, accuracy_list in accuracy_results.items():\n",
    "#    plt.plot(accuracy_list, label=model_type)\n",
    "#    #plt.violinplot(accuracy_list)\n",
    "#plt.legend()\n",
    "#y_min = min(min(accuracy_list) for accuracy_list in accuracy_results.values())\n",
    "#y_max = max(max(accuracy_list) for accuracy_list in accuracy_results.values()) \n",
    "#plt.ylim(y_min, y_max*1.2)\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodać powtarzalnośc wyników \n",
    "# (czyli dla każdego modelu ze znalezionymi hyperparametrami należy puścić uczenie 5 razy \n",
    "# i zobaczyć jaka jest średnia i odchykebue standarowe)\n",
    "\n",
    "# sprawdzić jakie klasy są najczęściej mylone i przygotować model \n",
    "# do rozpoznawania tylko tych mylących się klas. Połączyć następnie w całośc i sprawdzić wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing augmentation impact (na jakiejś jednej dowolnej klasie żeby sprawdzić różne warianty jak wpływają na wynik)\n",
    "# czyli np. \n",
    "# 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotować pretrenowane modele i sprawdzić wyniki (tutaj raczej nie trzeba wstawiać augmentacji danych).\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations variants (sprawdzanie która augmentacja ma największy wpływ)\n",
    "\n",
    "augmentation_random_flip =keras.Sequential([\n",
    "    RandomFlip(\"horizontal\")\n",
    "], name='augmentation_random_flip')\n",
    "\n",
    "augmentation_random_zoom =keras.Sequential([\n",
    "    RandomZoom(0.2)\n",
    "], name='augmentation_random_zoom')\n",
    "\n",
    "augmentation_random_rotation = keras.Sequential([\n",
    "    RandomRotation(0.2)\n",
    "], name='augmentation_random_rotation')\n",
    "\n",
    "\n",
    "augmentation_random_translation = keras.Sequential([\n",
    "    RandomTranslation(0.1, 0.1)\n",
    "], name='augmentation_random_translation')\n",
    "\n",
    "augmentation_random_contrast = keras.Sequential([\n",
    "    RandomContrast(0.1)\n",
    "], name='augmentation_random_contrast')\n",
    "\n",
    "augmentation_combined = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomZoom(0.2),\n",
    "    RandomRotation(0.2),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "    RandomContrast(0.1)\n",
    "], name='augmentation_combined')\n",
    "\n",
    "augmentation_random_flip_translation = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "], name='augmentation_random_flip_translation')\n",
    "\n",
    "augmentation_random_flip_translation_zoom = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "    RandomZoom(0.2)\n",
    "], name='augmentation_random_flip_translation_zoom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd752e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmentations = [\n",
    "    #augmentation_random_flip,\n",
    "    #augmentation_random_zoom,\n",
    "    #augmentation_random_rotation,\n",
    "    #augmentation_random_translation,\n",
    "    #augmentation_random_contrast,\n",
    "    augmentation_combined\n",
    "    #augmentation_random_flip_translation,\n",
    "    #augmentation_random_flip_translation_zoom\n",
    "]\n",
    "\n",
    "augmentation_results = {\n",
    "    augmentation.name + \"_\" + str_type: [] for augmentation in augmentations for str_type in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "print(augmentation_results)\n",
    "for augmentation in augmentations:\n",
    "    for replication in range(5):\n",
    "        model = keras.Sequential([\n",
    "        augmentation,\n",
    "        SimpleConvModel(n_filters_1=32,\n",
    "                       n_filters_2=32,\n",
    "                       dense_units_1=64)])\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        history = model.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=400, \n",
    "                            validation_data=(x_validation, y_validation), \n",
    "                            batch_size=256, \n",
    "                            callbacks=SimpleConvModel.get_callbacks()\n",
    "                            #callbacks=EarlyStopping(patience=50,monitor='val_loss')\n",
    "                           )\n",
    "        print(f\"augmentation name: {augmentation.name}\")\n",
    "        print(f\"iteration: {replication}\")\n",
    "        train_loss, train_accuracy = model.evaluate(x=x_train, y=y_train)\n",
    "        augmentation_results[augmentation.name + \"_train\"].append(train_accuracy)\n",
    "        validation_loss, validation_accuracy = model.evaluate(x=x_validation, y=y_validation)\n",
    "        augmentation_results[augmentation.name + \"_validation\"].append(validation_accuracy)\n",
    "        test_loss, test_accuracy = model.evaluate(x=x_test, y=y_test)\n",
    "        augmentation_results[augmentation.name + \"_test\"].append(test_accuracy)\n",
    "        print(f\"train accuracy: {train_accuracy}\")\n",
    "        print(f\"validation accuracy: {validation_accuracy}\")\n",
    "        print(f\"test accuracy: {test_accuracy}\")\n",
    "        plot_history(history, augmentation.name)\n",
    "        \n",
    "        print('============ NEW ITERATION ============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf710238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing augmentation results\n",
    "for augmentation_name, accuracy_list in augmentation_results.items():\n",
    "    avg = np.average(accuracy_list)\n",
    "    std = np.std(accuracy_list)\n",
    "    minimal = np.min(accuracy_list)\n",
    "    maximal = np.max(accuracy_list)\n",
    "    \n",
    "    print(f\"mode: {augmentation_name}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940edc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97490309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VISION transformer:\n",
    "num_classes = 10\n",
    "input_shape = (32, 32, 3)\n",
    "\n",
    "learning_rate = 0.001\n",
    "weight_decay = 0.0001\n",
    "batch_size = 126\n",
    "num_epochs = 100\n",
    "image_size = 72  # We'll resize input images to this size\n",
    "patch_size = 6  # Size of the patches to be extract from the input images\n",
    "num_patches = (image_size // patch_size) ** 2\n",
    "projection_dim = 64\n",
    "num_heads = 4\n",
    "transformer_units = [\n",
    "    projection_dim * 2,\n",
    "    projection_dim,\n",
    "]  # Size of the transformer layers\n",
    "transformer_layers = 8\n",
    "mlp_head_units = [2048, 1024]  # Size of the dense layers of the final classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data() # load unnormalized data\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.8)\n",
    "\n",
    "data_augmentation = keras.Sequential(\n",
    "    [\n",
    "        Normalization(),\n",
    "        Resizing(image_size, image_size),\n",
    "        RandomFlip(\"horizontal\"),\n",
    "        RandomRotation(factor=0.02),\n",
    "        RandomZoom(\n",
    "            height_factor=0.2, width_factor=0.2\n",
    "        ),\n",
    "    ],\n",
    "    name=\"data_augmentation\",\n",
    ")\n",
    "# Compute the mean and the variance of the training data for normalization.\n",
    "data_augmentation.layers[0].adapt(x_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a04d2044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement multilayer perceptron (MLP)\n",
    "def mlp(x, hidden_units, dropout_rate):\n",
    "    for units in hidden_units:\n",
    "        x = Dense(units, activation=tf.nn.gelu)(x)\n",
    "        x = Dropout(dropout_rate)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "# Implement patch creation as a layer\n",
    "class Patches(Layer):\n",
    "    def __init__(self, patch_size):\n",
    "        super().__init__()\n",
    "        self.patch_size = patch_size\n",
    "\n",
    "    def call(self, images):\n",
    "        batch_size = tf.shape(images)[0]\n",
    "        patches = tf.image.extract_patches(\n",
    "            images=images,\n",
    "            sizes=[1, self.patch_size, self.patch_size, 1],\n",
    "            strides=[1, self.patch_size, self.patch_size, 1],\n",
    "            rates=[1, 1, 1, 1],\n",
    "            padding=\"VALID\",\n",
    "        )\n",
    "        patch_dims = patches.shape[-1]\n",
    "        patches = tf.reshape(patches, [batch_size, -1, patch_dims])\n",
    "        return patches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d50129",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's display patches for a sample image\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(4, 4))\n",
    "image = x_train[np.random.choice(range(x_train.shape[0]))]\n",
    "plt.imshow(image.astype(\"uint8\"))\n",
    "plt.axis(\"off\")\n",
    "\n",
    "resized_image = tf.image.resize(\n",
    "    tf.convert_to_tensor([image]), size=(image_size, image_size)\n",
    ")\n",
    "patches = Patches(patch_size)(resized_image)\n",
    "print(f\"Image size: {image_size} X {image_size}\")\n",
    "print(f\"Patch size: {patch_size} X {patch_size}\")\n",
    "print(f\"Patches per image: {patches.shape[1]}\")\n",
    "print(f\"Elements per patch: {patches.shape[-1]}\")\n",
    "\n",
    "n = int(np.sqrt(patches.shape[1]))\n",
    "plt.figure(figsize=(4, 4))\n",
    "for i, patch in enumerate(patches[0]):\n",
    "    ax = plt.subplot(n, n, i + 1)\n",
    "    patch_img = tf.reshape(patch, (patch_size, patch_size, 3))\n",
    "    plt.imshow(patch_img.numpy().astype(\"uint8\"))\n",
    "    plt.axis(\"off\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05932900",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the patch encoding layer\n",
    "class PatchEncoder(Layer):\n",
    "    def __init__(self, num_patches, projection_dim):\n",
    "        super().__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.projection = Dense(units=projection_dim)\n",
    "        self.position_embedding = Embedding(\n",
    "            input_dim=num_patches, output_dim=projection_dim\n",
    "        )\n",
    "\n",
    "    def call(self, patch):\n",
    "        positions = tf.range(start=0, limit=self.num_patches, delta=1)\n",
    "        encoded = self.projection(patch) + self.position_embedding(positions)\n",
    "        return encoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16114b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vit_classifier():\n",
    "    inputs = Input(shape=input_shape)\n",
    "    # Augment data.\n",
    "    augmented = data_augmentation(inputs)\n",
    "    # Create patches.\n",
    "    patches = Patches(patch_size)(augmented)\n",
    "    # Encode patches.\n",
    "    encoded_patches = PatchEncoder(num_patches, projection_dim)(patches)\n",
    "\n",
    "    # Create multiple layers of the Transformer block.\n",
    "    for _ in range(transformer_layers):\n",
    "        # Layer normalization 1.\n",
    "        x1 = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "        # Create a multi-head attention layer.\n",
    "        attention_output = MultiHeadAttention(\n",
    "            num_heads=num_heads, key_dim=projection_dim, dropout=0.1\n",
    "        )(x1, x1)\n",
    "        # Skip connection 1.\n",
    "        x2 = Add()([attention_output, encoded_patches])\n",
    "        # Layer normalization 2.\n",
    "        x3 = LayerNormalization(epsilon=1e-6)(x2)\n",
    "        # MLP.\n",
    "        x3 = mlp(x3, hidden_units=transformer_units, dropout_rate=0.1)\n",
    "        # Skip connection 2.\n",
    "        encoded_patches = Add()([x3, x2])\n",
    "\n",
    "    # Create a [batch_size, projection_dim] tensor.\n",
    "    representation = LayerNormalization(epsilon=1e-6)(encoded_patches)\n",
    "    representation = Flatten()(representation)\n",
    "    representation = Dropout(0.5)(representation)\n",
    "    # Add MLP.\n",
    "    features = mlp(representation, hidden_units=mlp_head_units, dropout_rate=0.5)\n",
    "    # Classify outputs.\n",
    "    logits = Dense(num_classes)(features)\n",
    "    # Create the Keras model.\n",
    "    model = keras.Model(inputs=inputs, outputs=logits)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eeb6b3a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vit_results = {\"train\": [], \"validation\": [], \"test\": []}\n",
    "def run_experiment(model):\n",
    "    optimizer = tfa.optimizers.AdamW(\n",
    "        learning_rate=learning_rate, weight_decay=weight_decay\n",
    "    )\n",
    "\n",
    "    model.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "        metrics=[\n",
    "            keras.metrics.SparseCategoricalAccuracy(name=\"accuracy\"),\n",
    "            keras.metrics.SparseTopKCategoricalAccuracy(5, name=\"top-5-accuracy\"),\n",
    "        ],\n",
    "    )\n",
    "\n",
    "    checkpoint_filepath = \"models/vision_transformer\"\n",
    "    checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "        checkpoint_filepath,\n",
    "        monitor=\"val_accuracy\",\n",
    "        save_best_only=True,\n",
    "        # save_weights_only=True,\n",
    "    )\n",
    "    early_stopping_callback = keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_accuracy\",\n",
    "        patience=20\n",
    "    )\n",
    "\n",
    "    history = model.fit(\n",
    "        x=x_train,\n",
    "        y=y_train,\n",
    "        validation_data=(x_validation, y_validation),\n",
    "        batch_size=batch_size,\n",
    "        epochs=num_epochs,\n",
    "        #validation_split=0.1,\n",
    "        callbacks=[checkpoint_callback,early_stopping_callback],\n",
    "    )\n",
    "\n",
    "    #model.load_weights(checkpoint_filepath) # zmienic\n",
    "    model = keras.models.load_model(checkpoint_filepath)\n",
    "    \n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_train, y_train)\n",
    "    vit_results[\"train\"].append(accuracy)\n",
    "    print(f\"Train accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Train top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_validation, y_validation)\n",
    "    vit_results[\"validation\"].append(accuracy)\n",
    "    print(f\"Validation accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Validation top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "    _, accuracy, top_5_accuracy = model.evaluate(x_test, y_test)\n",
    "    vit_results[\"test\"].append(accuracy)\n",
    "    print(f\"Test accuracy: {round(accuracy * 100, 2)}%\")\n",
    "    print(f\"Test top 5 accuracy: {round(top_5_accuracy * 100, 2)}%\")\n",
    "\n",
    "    return history\n",
    "\n",
    "for i in range(5):\n",
    "    vit_classifier = create_vit_classifier()\n",
    "    history = run_experiment(vit_classifier)\n",
    "    plot_history(history, \"vision transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10789551",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_history(history, \"vision transformer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cfb7185",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyniki accuracy results\n",
    "for dataset_type, accuracy_list in vit_results.items():\n",
    "    avg = np.average(accuracy_list)\n",
    "    std = np.std(accuracy_list)\n",
    "    minimal = np.min(accuracy_list)\n",
    "    maximal = np.max(accuracy_list)\n",
    "    print(f\"dataset_type: {dataset_type}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2df8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vit_model = keras.models.load_model('models/vision_transformer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d47343bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "    _, accuracy, top_5_accuracy = vit_model.evaluate(x_train, y_train)\n",
    "    print(f\"Train accuracy: {accuracy}\")\n",
    "    print(f\"Train top 5 accuracy: {top_5_accuracy}\")\n",
    "    _, accuracy, top_5_accuracy = vit_model.evaluate(x_validation, y_validation)\n",
    "    print(f\"Validation accuracy: {accuracy}\")\n",
    "    print(f\"Validation top 5 accuracy: {top_5_accuracy}\")\n",
    "    _, accuracy, top_5_accuracy = vit_model.evaluate(x_test, y_test)\n",
    "    print(f\"Test accuracy: {accuracy}\")\n",
    "    print(f\"Test top 5 accuracy: {top_5_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fcef4cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = vit_model.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f5cebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a0faeef",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75a78a22",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_classes = np.argmax(predictions, axis=1)\n",
    "predicted_classes = predicted_classes[..., np.newaxis]\n",
    "predicted_classes.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687c8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "(predicted_classes == y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f12cc98",
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix(y_test,predicted_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7980415",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
