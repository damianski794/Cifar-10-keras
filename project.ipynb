{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c43498da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod, abstractstaticmethod\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.layers import (\n",
    "    Conv2D,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    Flatten,\n",
    "    Input,\n",
    "    MaxPool2D,\n",
    "    RandomContrast,\n",
    "    RandomFlip,\n",
    "    RandomRotation, # zamula\n",
    "    RandomTranslation,\n",
    "    RandomZoom\n",
    ")\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42e35b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if GPU is detected:\n",
    "len(tf.config.list_physical_devices('GPU')) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae1cc42d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# project:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "150ba766",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = keras.datasets.cifar10.load_data()\n",
    "x_train = x_train/255.0  # normalization\n",
    "x_test = x_test/255.0\n",
    "x_train, x_validation, y_train, y_validation = train_test_split(x_train, y_train, train_size=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e484ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_accuracy(history, model_name):\n",
    "    plt.plot(history.history['accuracy'])\n",
    "    plt.plot(history.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(history, model_name):\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'val'], loc='upper left')\n",
    "    plt.title(model_name)\n",
    "    plt.show()\n",
    "def plot_history(history, model_name):\n",
    "    plot_accuracy(history, model_name)\n",
    "    plot_loss(history, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "707736fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_names = {\n",
    "    0: \"airplane\",\n",
    "    1: \"automobile\",\n",
    "    2: \"bird\",\n",
    "    3: \"cat\",\n",
    "    4: \"deer\",\n",
    "    5: \"dog\",\n",
    "    6: \"frog\",\n",
    "    7: \"horse\",\n",
    "    8: \"ship\",\n",
    "    9: \"truck\",\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26afa803",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HPConfiguration(ABC):\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def build_hp_model(hp):\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_callbacks():\n",
    "        pass\n",
    "    \n",
    "    @abstractstaticmethod\n",
    "    def get_tuner_callbacks():\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf00944c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# neural networks\n",
    "\n",
    "class SuperSimpleConvModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters, \n",
    "                 dense_units,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "        \n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "   \n",
    "    def build_hp_model(hp):\n",
    "        n_filters = hp.Int(\"n_filters\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units = hp.Int(\"dense_units\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SuperSimpleConvModel(\n",
    "            n_filters=n_filters, dense_units=dense_units\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SuperSimpleConvModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SuperSimpleConvModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SuperSimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]\n",
    "    \n",
    "    def get_best_model():\n",
    "        tuner = SuperSimpleConvModel.get_tuner()\n",
    "        best_parameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "        return SuperSimpleConvModel.build_hp_model(best_parameters)\n",
    "    \n",
    "    \n",
    "####################\n",
    "class SimpleConvModel(keras.Model, HPConfiguration):\n",
    "    def __init__(self, \n",
    "                 n_filters_1,\n",
    "                 n_filters_2,\n",
    "                 dense_units_1,\n",
    "                 **kwargs):\n",
    "        super().__init__(kwargs)\n",
    "\n",
    "        self.conv2D_1 = Conv2D(filters=n_filters_1,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_1 = MaxPool2D(pool_size=(2,2))\n",
    "        self.conv2D_2 = Conv2D(filters=n_filters_2,kernel_size=(4,4),input_shape=(32,32,3),activation='relu')\n",
    "        self.max_pool2D_2 = MaxPool2D(pool_size=(2,2))\n",
    "        self.flatten = Flatten()\n",
    "        self.dense_1 = Dense(dense_units_1,activation='relu')\n",
    "        self.dense_2 = Dense(10,activation='softmax')\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        #print(inputs)\n",
    "        x =  self.conv2D_1(inputs)\n",
    "        x = self.max_pool2D_1(x)\n",
    "        x = self.conv2D_2(x)\n",
    "        x = self.max_pool2D_2(x)\n",
    "\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense_1(x)\n",
    "        x = self.dense_2(x)\n",
    "        return x\n",
    "\n",
    "    def build_hp_model(hp):\n",
    "        n_filters_1 = hp.Int(\"n_filters_1\", min_value=4, max_value=32, step=2, sampling=\"log\")\n",
    "        n_filters_2 = hp.Int(\"n_filters_2\", min_value=2, max_value=32, step=2, sampling=\"log\")\n",
    "        dense_units_1 = hp.Int(\"dense_units_1\", min_value=16, max_value=256, step=2, sampling=\"log\")\n",
    "        model = SimpleConvModel(\n",
    "            n_filters_1=n_filters_1, n_filters_2=n_filters_2, dense_units_1=dense_units_1\n",
    "        )\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        return model\n",
    "\n",
    "    def get_tuner():\n",
    "        return kt.Hyperband(SimpleConvModel.build_hp_model,\n",
    "                            objective='val_accuracy',\n",
    "                            #overwrite=True,\n",
    "                            max_epochs=50,\n",
    "                            factor=3,\n",
    "                            directory='tuner/SimpleConvModel',\n",
    "                            project_name='model'\n",
    "                           )\n",
    "    def get_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "            #tf.keras.callbacks.ModelCheckpoint(filepath=\"models/SimpleConvModel\", save_best_only=True)\n",
    "        ]\n",
    "    \n",
    "    def get_tuner_callbacks():\n",
    "        return [\n",
    "            tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20),\n",
    "        ]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd1ee4c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41d34a1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_classes = [SuperSimpleConvModel, SimpleConvModel] # tutaj dorzuć trzeci model do listy\n",
    "\n",
    "# HYPERPARAMETER TUNING\n",
    "for model_class in model_classes:\n",
    "    print(model_class)\n",
    "    # break\n",
    "    tuner = model_class.get_tuner()\n",
    "    #print(tuner)\n",
    "    tuner.search(x_train, y_train, batch_size=256, epochs=50, validation_data=(x_validation, y_validation), callbacks=model_class.get_tuner_callbacks())\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0068837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# accuracy for every model (for training, validation and test set)\n",
    "accuracy_results = {\n",
    "    model_class.__name__ + \"_\" + str_type: [] for model_class in model_classes for str_type in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "\n",
    "print(accuracy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a96979b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a2cebfc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_classes = [SimpleConvModel]\n",
    "#model_classes = [SuperSimpleConvModel, SimpleConvModel]\n",
    "\n",
    "for model_class in model_classes:\n",
    "    tuner = model_class.get_tuner()\n",
    "    best_hps=tuner.get_best_hyperparameters(num_trials=1)[0]\n",
    "    print(best_hps.values)\n",
    "    #best_hps.\n",
    "    \n",
    "    \n",
    "    for replication in range(4):\n",
    "        print(f'Starting {replication} iteration')\n",
    "        best_model = model_class.build_hp_model(best_hps)\n",
    "        print(best_model)\n",
    "        history = best_model.fit(x_train, y_train, epochs=200, validation_data=(x_validation, y_validation), batch_size=256, callbacks=model_class.get_callbacks(), verbose=0)\n",
    "   # TODO: dorzucić wczytywanie wag najlepszego modelu (optymalizacja pod kątem max validation_accuracy)     \n",
    "        print(f\"model name: {model_class.__name__}\")\n",
    "        print(f\"iteration: {replication}\")\n",
    "        train_loss, train_accuracy = best_model.evaluate(x=x_train, y=y_train)\n",
    "        accuracy_results[model_class.__name__ + \"_train\"].append(train_accuracy)\n",
    "        validation_loss, validation_accuracy = best_model.evaluate(x=x_validation, y=y_validation)\n",
    "        accuracy_results[model_class.__name__ + \"_validation\"].append(validation_accuracy)\n",
    "        test_loss, test_accuracy = best_model.evaluate(x=x_test, y=y_test)\n",
    "        accuracy_results[model_class.__name__ + \"_test\"].append(test_accuracy)\n",
    "        print(f\"train accuracy: {train_accuracy}\")\n",
    "        print(f\"validation accuracy: {validation_accuracy}\")\n",
    "        print(f\"test accuracy: {test_accuracy}\")\n",
    "        plot_history(history, model_class.__name__)\n",
    "        \n",
    "        print('============ NEW ITERATION ============')\n",
    "\n",
    "print(accuracy_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "274b0ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# wyniki accuracy results\n",
    "for model_type, accuracy_list in accuracy_results.items():\n",
    "    avg = np.average(accuracy_list)\n",
    "    std = np.std(accuracy_list)\n",
    "    minimal = np.min(accuracy_list)\n",
    "    maximal = np.max(accuracy_list)\n",
    "    print(f\"mode: {model_type}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "    print('==============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f4e471",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for model_type, accuracy_list in accuracy_results.items():\n",
    "#    plt.plot(accuracy_list, label=model_type)\n",
    "#    #plt.violinplot(accuracy_list)\n",
    "#plt.legend()\n",
    "#y_min = min(min(accuracy_list) for accuracy_list in accuracy_results.values())\n",
    "#y_max = max(max(accuracy_list) for accuracy_list in accuracy_results.values()) \n",
    "#plt.ylim(y_min, y_max*1.2)\n",
    "##plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9bf3f6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dodać powtarzalnośc wyników \n",
    "# (czyli dla każdego modelu ze znalezionymi hyperparametrami należy puścić uczenie 5 razy \n",
    "# i zobaczyć jaka jest średnia i odchykebue standarowe)\n",
    "\n",
    "# sprawdzić jakie klasy są najczęściej mylone i przygotować model \n",
    "# do rozpoznawania tylko tych mylących się klas. Połączyć następnie w całośc i sprawdzić wyniki"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02dca971",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing augmentation impact (na jakiejś jednej dowolnej klasie żeby sprawdzić różne warianty jak wpływają na wynik)\n",
    "# czyli np. \n",
    "# 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "248fbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# przygotować pretrenowane modele i sprawdzić wyniki (tutaj raczej nie trzeba wstawiać augmentacji danych).\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fa5f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# augmentations variants (sprawdzanie która augmentacja ma największy wpływ)\n",
    "\n",
    "augmentation_random_flip =keras.Sequential([\n",
    "    RandomFlip(\"horizontal\")\n",
    "], name='augmentation_random_flip')\n",
    "\n",
    "augmentation_random_zoom =keras.Sequential([\n",
    "    RandomZoom(0.2)\n",
    "], name='augmentation_random_zoom')\n",
    "\n",
    "augmentation_random_rotation = keras.Sequential([\n",
    "    RandomRotation(0.2)\n",
    "], name='augmentation_random_rotation')\n",
    "\n",
    "\n",
    "augmentation_random_translation = keras.Sequential([\n",
    "    RandomTranslation(0.1, 0.1)\n",
    "], name='augmentation_random_translation')\n",
    "\n",
    "augmentation_random_contrast = keras.Sequential([\n",
    "    RandomContrast(0.1)\n",
    "], name='augmentation_random_contrast')\n",
    "\n",
    "augmentation_combined = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomZoom(0.2),\n",
    "    RandomRotation(0.2),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "    RandomContrast(0.1)\n",
    "], name='augmentation_combined')\n",
    "\n",
    "augmentation_random_flip_translation = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "], name='augmentation_random_flip_translation')\n",
    "\n",
    "augmentation_random_flip_translation_zoom = keras.Sequential([\n",
    "    RandomFlip(\"horizontal\"),\n",
    "    RandomTranslation(0.1, 0.1),\n",
    "    RandomZoom(0.2)\n",
    "], name='augmentation_random_flip_translation_zoom')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd752e49",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmentations = [\n",
    "    #augmentation_random_flip,\n",
    "    #augmentation_random_zoom,\n",
    "    #augmentation_random_rotation,\n",
    "    #augmentation_random_translation,\n",
    "    #augmentation_random_contrast,\n",
    "    augmentation_combined\n",
    "    #augmentation_random_flip_translation,\n",
    "    #augmentation_random_flip_translation_zoom\n",
    "]\n",
    "\n",
    "augmentation_results = {\n",
    "    augmentation.name + \"_\" + str_type: [] for augmentation in augmentations for str_type in [\"train\", \"validation\", \"test\"]\n",
    "}\n",
    "print(augmentation_results)\n",
    "for augmentation in augmentations:\n",
    "    for replication in range(5):\n",
    "        model = keras.Sequential([\n",
    "        augmentation,\n",
    "        SimpleConvModel(n_filters_1=32,\n",
    "                       n_filters_2=32,\n",
    "                       dense_units_1=64)])\n",
    "        model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "        history = model.fit(x_train, \n",
    "                            y_train, \n",
    "                            epochs=400, \n",
    "                            validation_data=(x_validation, y_validation), \n",
    "                            batch_size=256, \n",
    "                            callbacks=SimpleConvModel.get_callbacks()\n",
    "                            #callbacks=EarlyStopping(patience=50,monitor='val_loss')\n",
    "                           )\n",
    "        print(f\"augmentation name: {augmentation.name}\")\n",
    "        print(f\"iteration: {replication}\")\n",
    "        train_loss, train_accuracy = model.evaluate(x=x_train, y=y_train)\n",
    "        augmentation_results[augmentation.name + \"_train\"].append(train_accuracy)\n",
    "        validation_loss, validation_accuracy = model.evaluate(x=x_validation, y=y_validation)\n",
    "        augmentation_results[augmentation.name + \"_validation\"].append(validation_accuracy)\n",
    "        test_loss, test_accuracy = model.evaluate(x=x_test, y=y_test)\n",
    "        augmentation_results[augmentation.name + \"_test\"].append(test_accuracy)\n",
    "        print(f\"train accuracy: {train_accuracy}\")\n",
    "        print(f\"validation accuracy: {validation_accuracy}\")\n",
    "        print(f\"test accuracy: {test_accuracy}\")\n",
    "        plot_history(history, augmentation.name)\n",
    "        \n",
    "        print('============ NEW ITERATION ============')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207e4f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "augmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf710238",
   "metadata": {},
   "outputs": [],
   "source": [
    "# printing augmentation results\n",
    "for augmentation_name, accuracy_list in augmentation_results.items():\n",
    "    avg = np.average(accuracy_list)\n",
    "    std = np.std(accuracy_list)\n",
    "    minimal = np.min(accuracy_list)\n",
    "    maximal = np.max(accuracy_list)\n",
    "    \n",
    "    print(f\"mode: {augmentation_name}, avg: {avg}, std: {std}, min: {minimal}, max: {maximal}\")\n",
    "    print('===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a940edc5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "augmentation_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97490309",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bed5172d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
